{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref = https://tykimos.github.io/2017/01/27/CNN_Layer_Talk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2D(32, (5, 5), padding='valid', input_shape=(28, 28, 1), activation='relu')\n",
    "# 첫번째 인자 : 컨볼루션 필터의 수 입니다.\n",
    "# 두번째 인자 : 컨볼루션 커널의 (행, 열) 입니다.\n",
    "# padding : 경계 처리 방법을 정의합니다.\n",
    "#   ‘valid’ : 유효한 영역만 출력이 됩니다. 따라서 출력 이미지 사이즈는 입력 사이즈보다 작습니다.\n",
    "#   ‘same’ : 출력 이미지 사이즈가 입력 이미지 사이즈와 동일합니다.\n",
    "# input_shape : 샘플 수를 제외한 입력 형태를 정의 합니다. 모델에서 첫 레이어일 때만 정의하면 됩니다.\n",
    "#   (행, 열, 채널 수)로 정의합니다. 흑백영상인 경우에는 채널이 1이고, 컬러(RGB)영상인 경우에는 채널을 3으로 설정합니다.\n",
    "# activation : 활성화 함수 설정합니다.\n",
    "#   ‘linear’ : 디폴트 값, 입력뉴런과 가중치로 계산된 결과값이 그대로 출력으로 나옵니다.\n",
    "#   ‘relu’ : rectifier 함수, 은익층에 주로 쓰입니다.\n",
    "#   ‘sigmoid’ : 시그모이드 함수, 이진 분류 문제에서 출력층에 주로 쓰입니다.\n",
    "#   ‘softmax’ : 소프트맥스 함수, 다중 클래스 분류 문제에서 출력층에 주로 쓰입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root@32b86b4f803f:~# cat ~/.keras/keras.json\n",
    "# {\n",
    "#     \"epsilon\": 1e-07,\n",
    "#     \"image_data_format\": \"channels_last\",\n",
    "#     \"backend\": \"tensorflow\",\n",
    "#     \"floatx\": \"float32\"\n",
    "# }\n",
    "\n",
    "# 입력 형태는\n",
    "# image_data_format이 ‘channels_last’인 경우 (샘플 수, 행, 열, 채널 수)로 이루어진 4D 텐서입니다.\n",
    "# image_data_format이 ‘channels_first’인 경우 (샘플 수, 채널 수, 행, 열)로 이루어진 4D 텐서입니다.\n",
    "\n",
    "# 출력 형태는\n",
    "# image_data_format이 ‘channels_last’인 경우 (샘플 수, 행, 열, 필터 수)로 이루어진 4D 텐서입니다.\n",
    "# image_data_format이 ‘channels_first’인 경우 (샘플 수, 필터 수, 행, 열)로 이루어진 4D 텐서입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution filter count = 1\n",
    "# convolution filter size = 2x2\n",
    "# padding = valid\n",
    "# input_shape = 3row 3column 1channel\n",
    "# Conv2D(1, (2,2), padding='valid', input_shape=(3, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번 그림으로 Conv2D 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 뉴런이 9개 출력 뉴런이 4개이면 \n",
    "# Conv2D(1, (2,2), padding='valid', input_shape=(3, 3, 1)) 의 weight의 count = 4\n",
    "# Dense(4, input_dim=9) 의 weight의 count = 9 * 4 = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2번 그림으로 Conv2D 와 Dense 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution 의 특징\n",
    "# 1 파라미터 공유 = \n",
    "#   하나의 필터로 입력 이미지를 순회하기 때문에 순회할 때 적용되는 가중치는 모두 동일합니다. \n",
    "#   이는 학습해야할 가중치 수를 현저하게 줄여줍니다.\n",
    "# 2 지역적인 특징을 인식 =\n",
    "#   출력에 영향을 미치는 영역이 지역적으로 제한되어 있습니다. \n",
    "#   즉 그림에서 y~0~에 영향을 미치는 입력은 x~0~, x~1~, x~3~, x~4~으로 한정되어 있습니다. \n",
    "#   이는 영상 인식에 적합합니다. 예를 들어 코를 볼 때는 코 주변만 보고, 눈을 볼 때는 눈 주변만 보면서 학습 및 인식하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 padding\n",
    "#   ‘valid’인 경우에는 입력 이미지 영역에 맞게 필터를 적용하기 때문에 출력 이미지 크기가 입력 이미지 크기보다 작아집니다.\n",
    "#   ‘same’은 출력 이미지와 입력 이미지 사이즈가 동일하도록 입력 이미지 경계에 빈 영역을 추가하여 필터를 적용합니다.\n",
    "#   'same’으로 설정 시, 입력 이미지에 경계를 학습시키는 효과가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3번 그림으로 padding 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Convolution filter count\n",
    "#    Conv2D(1, (2,2), padding='same', input_shape=(3,3,1))\n",
    "#    Conv2D(3, (2,2), padding='same', input_shape=(3,3,1))\n",
    "#    필터가 3개라서 출력 이미지도 필터 수에 따라 3개로 늘어났습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4~7번 그림으로 필터 갯수 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   입력 이미지 사이즈가 3 x 3 입니다.\n",
    "#   2 x 2 커널을 가진 필터가 3개입니다. 가중치는 총 12개 입니다.\n",
    "#   출력 이미지 사이즈가 3 x 3이고 총 3개입니다. 이는 채널이 3개다라고도 표현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Convolution Channel Count\n",
    "#    Conv2D(1, (2,2), padding='same', input_shape=(3,3,1))\n",
    "#    Conv2D(1, (2,2), padding='same', input_shape=(3,3,3))\n",
    "#    필터가 1개라서 출력 이미지도 필터 수에 따라 1개로 줄었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, 8번 그림 으로 채널 수 확인하기\n",
    "# 직관적으로 이해하기 힘들다 \n",
    "# -> 컬러사진 1장에 흑백필터를 사용하면 결과가 1장의 사진이 나온다.\n",
    "# -> 흑백사진 1장에 흑백필터를 사용하면 결과가 1장의 사진이 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   입력 이미지 사이즈가 3 x 3 이고, 채널이 3 개 입니다.\n",
    "#   2 x 2 커널을 가진 필터가 1개입니다. 채널마다 같은 커널이 할당되어 총 가중치는 12개 입니다.\n",
    "#   출력 이미지는 사이즈 3 x 3 이고 채널이 1 개 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 이미지의 사이즈가 3 x 3 이고, 채널이 3개 이고,\n",
    "# 사이즈가 2 x 2인 필터가 2개인 Convolution\n",
    "# Conv2D(2, (2,2), padding='same', input_shape=(3, 3, 3))\n",
    "# 채널마다 가중치가 할당되어 총 가중치는 3 x 2 x 2 x 2 = 24 개 입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
